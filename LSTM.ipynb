{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.read_pickle('D:/ml_data/data_lstm_full.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "columns = [column for column in matrix.columns if column not in ['date_block_num', 'shop_id', 'item_id', 'item_cnt_month']]\n",
    "\n",
    "matrix.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "matrix.fillna(0, inplace=True)\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "matrix[columns] = scaler_X.fit_transform(matrix[columns].values)\n",
    "\n",
    "# scaler_y = MinMaxScaler()\n",
    "# matrix['item_cnt_month'] = scaler_y.fit_transform(matrix['item_cnt_month'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HWer\\Anaconda3\\envs\\ml_project\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['scaler_X_2.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(scaler_X, 'scaler_X_2.pkl')\n",
    "# joblib.dump(scaler_y, 'scaler_y.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.to_pickle('D:/ml_data/data_lstm_X_scaled.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.read_pickle('D:/ml_data/data_lstm_X_scaled.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_stack(a, stepsize=1, width=2):\n",
    "    n = a.shape[1]\n",
    "    omg = lambda i: 1+n+i-width\n",
    "    _X = [a[:, i:i+width].astype(np.float16) for i in range(0, n - width)]\n",
    "    _y = [a[:, i].astype(np.float16) for i in range(width, n)]\n",
    "    del a\n",
    "    _X = np.stack(_X, axis=1).astype(np.float16)\n",
    "    _y = np.stack(_y, axis=1).astype(np.float16)\n",
    "\n",
    "    return _X.reshape(-1, _X.shape[2], _X.shape[3]), _y.reshape(-1, _y.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(data, width=7):\n",
    "    data.fillna(0, inplace=True)\n",
    "    \n",
    "    test_data = data[(data.date_block_num >= (33 - width)) & (data.date_block_num < 34)]\n",
    "    X = np.array(list(test_data.groupby(['shop_id', 'item_id']).apply(pd.DataFrame.as_matrix)))\n",
    "    del test_data\n",
    "    \n",
    "    X, y = window_stack(X, width=width)\n",
    "    test_X = X[:, :, 3:]\n",
    "    test_y = y[:, 3]\n",
    "    del X\n",
    "    del y\n",
    "    \n",
    "    train_data = data[data.date_block_num < 33]\n",
    "    del data\n",
    "    \n",
    "    X = np.array(list(train_data.groupby(['shop_id', 'item_id']).apply(pd.DataFrame.as_matrix)))\n",
    "    del train_data\n",
    "    \n",
    "    X, y = window_stack(X, width=width)\n",
    "    train_X = X[:, :, 3:]\n",
    "    train_y = y[:, 3]\n",
    "    del X\n",
    "    del y\n",
    "        \n",
    "    return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "widths = [7, 14, 21, 28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HWer\\Anaconda3\\envs\\ml_project\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:942: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  return reduction.apply_frame_axis0(sdata, f, names, starts, ends)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float16\n",
      "214.28668212890625\n",
      "float16\n",
      "321.5605342388153\n",
      "float16\n",
      "412.7312242984772\n"
     ]
    }
   ],
   "source": [
    "for width in widths:\n",
    "    ts = time.time()\n",
    "    train_X, train_y, test_X, test_y = get_X_y(matrix, width)\n",
    "    \n",
    "    train_data_file = 'train_data_Xscaled_{}.npz'.format(width)\n",
    "    test_data_file = 'test_data_Xscaled_{}.npz'.format(width)\n",
    "    \n",
    "    np.savez_compressed(train_data_file, X=train_X, y=train_y)\n",
    "    np.savez_compressed(test_data_file, X=test_X, y=test_y)\n",
    "    \n",
    "    print(train_X.dtype)\n",
    "    del train_X\n",
    "    del train_y\n",
    "    del test_X\n",
    "    del test_y\n",
    "    \n",
    "    print(time.time() - ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-87b67d82029d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_X' is not defined"
     ]
    }
   ],
   "source": [
    "print(train_X.shape)\n",
    "print(train_y.shape)\n",
    "print(test_X.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD, Adagrad, Adadelta, RMSprop, Adam, Adamax, Nadam\n",
    "# from Eve.Eve import Eve\n",
    "\n",
    "\n",
    "__name_to_optimizer = {\n",
    "    'rmsprop': RMSprop,\n",
    "    'adam': Adam,\n",
    "    'sgd': SGD,\n",
    "    'adagrad': Adagrad,\n",
    "    'adadelta': Adadelta,\n",
    "    'adamax': Adamax,\n",
    "    'nadam': Nadam,\n",
    "    # 'eve': Eve,\n",
    "}\n",
    "\n",
    "\n",
    "def make_optimizer(optimizer_name='adam', **optimizer_params):\n",
    "    try:\n",
    "        optimizer = __name_to_optimizer[optimizer_name]\n",
    "    except KeyError as e:\n",
    "        raise ValueError('Undefined optimizer: {}'.format(e.args[0]))\n",
    "\n",
    "    return optimizer(**optimizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import l1_l2\n",
    "\n",
    "\n",
    "__name_to_loss = {\n",
    "    'mse': 'mse',\n",
    "    'rmse': root_mean_squared_error,\n",
    "}\n",
    "\n",
    "def make_loss(loss_name):\n",
    "    return __name_to_loss[loss_name]\n",
    "\n",
    "    \n",
    "class LSTMWrapper(object):\n",
    "    def __init__(self, features_count, units, dropout_rate, regularization_param, epochs_count, loss, optimizer):\n",
    "        self.features_count = features_count\n",
    "        self.units = units\n",
    "#         self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.regularizer = l1_l2(l1=regularization_param, l2=regularization_param)\n",
    "        self.epochs_count = epochs_count\n",
    "        self.loss = make_loss(loss)\n",
    "        self.optimizer_name = optimizer\n",
    "        self.initializer = 'glorot_normal'\n",
    "        self.learning_rate=0.0001\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        lstm_layer = LSTM(\n",
    "            units=self.units,\n",
    "            input_shape=(self.features_count, 75),\n",
    "            kernel_regularizer=self.regularizer,\n",
    "            kernel_initializer=self.initializer,\n",
    "        )\n",
    "        \n",
    "        model.add(lstm_layer)\n",
    "#         model.add(BatchNormalization())\n",
    "        model.add(Dropout(self.dropout_rate))\n",
    "\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        opt = make_optimizer(self.optimizer_name, lr=self.learning_rate)\n",
    "        model.compile(loss=self.loss, optimizer=opt, metrics=[root_mean_squared_error])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def score_sklearn(self, test_data, metrics, scaler=None):\n",
    "        if metrics is str:\n",
    "            metrics = [metrics]\n",
    "\n",
    "        y_preds = self.model.predict(test_data[0])\n",
    "        ys = test_data[1]\n",
    "\n",
    "        scores = dict()\n",
    "        for metric_name in metrics:\n",
    "            scores['predictor_{}'.format(metric_name)] = make_sklearn_metric(metric_name)(ys, y_preds)\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "model_params = dict(\n",
    "    features_count = 33,\n",
    "    units=64,\n",
    "    dropout_rate=0.5,\n",
    "    regularization_param=1e-3,\n",
    "    epochs_count=200,\n",
    "    loss='rmse',\n",
    "    optimizer='adam',\n",
    ")\n",
    "lstm = LSTMWrapper(**model_params)\n",
    "# lstm.model.fit(X_train, y_train, batch_size = 4096, epochs = 1, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.read_pickle('D:/ml_data/data_lstm_X_scaled.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid, ParameterSampler\n",
    "\n",
    "param_test = dict(\n",
    "    features_count=[33], #[7, 14, 21, 28]\n",
    "    units=[16, 32, 64, 128],\n",
    "    dropout_rate=[0.1, 0.25, 0.5],\n",
    "    regularization_param=[10 ** -i for i in range(3, 7)],\n",
    "    epochs_count=[100],\n",
    "    loss=['mse', 'rmse'],\n",
    "    optimizer=['adam', 'rmsprop'],\n",
    ")\n",
    "\n",
    "sampler = ParameterSampler(param_test, n_iter=25, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models_dir = 'trained_models_dir'\n",
    "trained_models_dir = trained_models_dir + '/lstm'\n",
    "if not os.path.exists(trained_models_dir):\n",
    "    os.makedirs(trained_models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inner_dirs(path):\n",
    "    for file in os.listdir(path):\n",
    "        if os.path.isdir(os.path.join(path, file)):\n",
    "            yield file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __create_model_directory(models_dir) -> str:\n",
    "    inner_dirs = list(get_inner_dirs(models_dir))\n",
    "    folder_name = 'gs_{}'.format(len(inner_dirs))\n",
    "\n",
    "    model_dir = os.path.join(models_dir, folder_name)\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "\n",
    "    return model_dir, folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y(data):\n",
    "    #     data = data[data.date_block_num > 11]\n",
    "    X = data.drop(['date_block_num', 'item_cnt_month', 'shop_id', 'item_id'], axis=1)\n",
    "    y = data['item_cnt_month']\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def rmse(y_actual, y_predicted):\n",
    "    return sqrt(mean_squared_error(y_actual, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = []\n",
    "search_results_file = os.path.join(trained_models_dir, 'search_results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HWer\\Anaconda3\\envs\\ml_project\\lib\\site-packages\\pandas\\core\\groupby\\ops.py:942: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  return reduction.apply_frame_axis0(sdata, f, names, starts, ends)\n"
     ]
    }
   ],
   "source": [
    "def get_X_y_train(data):\n",
    "    X = data[data.date_block_num < 33]\n",
    "    X = np.array(list(X.groupby(['shop_id', 'item_id']).apply(pd.DataFrame.as_matrix)))\n",
    "    X = X[:, :, 3:]\n",
    "    y = data[data.date_block_num == 33]['item_cnt_month']\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X, y = get_X_y_train(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current parameters : {'units': 16, 'regularization_param': 1e-06, 'optimizer': 'adam', 'loss': 'rmse', 'features_count': 33, 'epochs_count': 100, 'dropout_rate': 0.5}\n",
      "model directory : trained_models_dir/lstm\\gs_7\n",
      "Train on 171360 samples, validate on 42840 samples\n",
      "Epoch 1/5\n",
      "171360/171360 [==============================] - 76s 445us/step - loss: 2.6075 - root_mean_squared_error: 2.7586 - val_loss: 2.6529 - val_root_mean_squared_error: 2.6524\n",
      "Epoch 2/5\n",
      "171360/171360 [==============================] - 44s 256us/step - loss: 2.5233 - root_mean_squared_error: 2.7065 - val_loss: 2.6522 - val_root_mean_squared_error: 2.6516\n",
      "Epoch 3/5\n",
      "171360/171360 [==============================] - 17s 98us/step - loss: 2.6075 - root_mean_squared_error: 2.6798 - val_loss: 2.6515 - val_root_mean_squared_error: 2.6509\n",
      "Epoch 4/5\n",
      "171360/171360 [==============================] - 12s 67us/step - loss: 2.4995 - root_mean_squared_error: 2.3800 - val_loss: 2.6508 - val_root_mean_squared_error: 2.6502\n",
      "Epoch 5/5\n",
      "171360/171360 [==============================] - 12s 68us/step - loss: 2.6784 - root_mean_squared_error: 2.7325 - val_loss: 2.6501 - val_root_mean_squared_error: 2.6495\n",
      "overwrite model parameters file (trained_models_dir/lstm\\search_results.json)\n",
      "current parameters : {'units': 64, 'regularization_param': 1e-05, 'optimizer': 'rmsprop', 'loss': 'mse', 'features_count': 33, 'epochs_count': 100, 'dropout_rate': 0.1}\n",
      "model directory : trained_models_dir/lstm\\gs_8\n",
      "Train on 171360 samples, validate on 42840 samples\n",
      "Epoch 1/5\n",
      "171360/171360 [==============================] - 355s 2ms/step - loss: 7.3447 - root_mean_squared_error: 2.5523 - val_loss: 7.1663 - val_root_mean_squared_error: 2.6745\n",
      "Epoch 2/5\n",
      "171360/171360 [==============================] - 140s 818us/step - loss: 7.2972 - root_mean_squared_error: 2.7292 - val_loss: 7.1475 - val_root_mean_squared_error: 2.6710\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-043d93f6978c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m#         del valid_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mlstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m65536\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mlstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lstm.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml_project\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml_project\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml_project\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml_project\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml_project\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml_project\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml_project\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml_project\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for params in list(sampler):\n",
    "        print('current parameters : {}'.format(params))\n",
    "\n",
    "        model_dir, folder_name = __create_model_directory(trained_models_dir)\n",
    "        print('model directory : {}'.format(model_dir))\n",
    "        lstm = LSTMWrapper(**params)\n",
    "        \n",
    "#         width = params['features_count']\n",
    "#         train_data_file = 'train_data_Xscaled_{}.npz'.format(width)\n",
    "#         valid_data_file = 'test_data_Xscaled_{}.npz'.format(width)\n",
    "        \n",
    "#         train_data = np.load(train_data_file)\n",
    "#         X_train = train_data['X'].astype(np.float16)\n",
    "#         y_train = train_data['y'].astype(np.float16)\n",
    "#         if X_train.shape[0] > 1200000:\n",
    "#             n = 1200000\n",
    "#             index = np.random.choice(X.shape[0], n, replace=False)  \n",
    "#             X_train = X_train[index]\n",
    "#             y_train = y_train[index]\n",
    "#         del train_data\n",
    "        \n",
    "#         valid_data = np.load(valid_data_file)\n",
    "#         X_valid = valid_data['X'].astype(np.float16)\n",
    "#         y_valid = valid_data['y'].astype(np.float16)\n",
    "#         del valid_data\n",
    "        \n",
    "        lstm.model.fit(X_train, y_train, batch_size = 65536, epochs = 5, validation_data=(X_valid, y_valid))\n",
    "        lstm.model.save(os.path.join(model_dir, 'lstm.h5'))\n",
    "        \n",
    "        \n",
    "        mean_cv_scores = [\n",
    "            rmse(y_train, lstm.model.predict(X_train)),\n",
    "            rmse(y_valid, lstm.model.predict(X_valid)),\n",
    "#             rmse(test_y, lstm.model.predict(test_X)),\n",
    "        ]\n",
    "        \n",
    "        write_message = dict(\n",
    "            params=params,\n",
    "            scores=dict(\n",
    "                train=mean_cv_scores[0],\n",
    "                val=mean_cv_scores[1],\n",
    "#                 test=mean_cv_scores[2],\n",
    "            ),\n",
    "            model_path=model_dir,\n",
    "        )\n",
    "\n",
    "        search_results.append(write_message)\n",
    "\n",
    "        with open(search_results_file, 'w') as file:\n",
    "            json.dump(search_results, file)\n",
    "            print('overwrite model parameters file ({})'.format(search_results_file))\n",
    "            \n",
    "#         del X_train\n",
    "#         del y_train\n",
    "        \n",
    "#         del X_valid\n",
    "#         del y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(search_results_file, 'r') as file:\n",
    "    search_results = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scores = [record['scores']['val'] for record in search_results]\n",
    "min_index = val_scores.index(min(val_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'units': 16,\n",
       "  'regularization_param': 1e-06,\n",
       "  'optimizer': 'adam',\n",
       "  'loss': 'rmse',\n",
       "  'features_count': 33,\n",
       "  'epochs_count': 100,\n",
       "  'dropout_rate': 0.5},\n",
       " 'scores': {'train': 2.67440765097046, 'val': 2.6495462532966196},\n",
       " 'model_path': 'trained_models_dir/lstm\\\\gs_7'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = search_results[min_index]\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(os.path.join(best_model['model_path'], 'lstm.h5'), custom_objects={'root_mean_squared_error': root_mean_squared_error})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = best_model['params']['features_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file = 'train_data_Xscaled_{}.npz'.format(width)\n",
    "valid_data_file = 'test_data_Xscaled_{}.npz'.format(width)\n",
    "\n",
    "train_data = np.load(train_data_file)\n",
    "X_train = train_data['X'].astype(np.float16)\n",
    "y_train = train_data['y'].astype(np.float16)\n",
    "del train_data\n",
    "\n",
    "valid_data = np.load(valid_data_file)\n",
    "X_valid = valid_data['X'].astype(np.float16)\n",
    "y_valid = valid_data['y'].astype(np.float16)\n",
    "del valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.7020194378202387, 2.7225219168012917]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred = model.predict(X_train)\n",
    "valid_pred = model.predict(X_valid)\n",
    "\n",
    "# y_train = scaler_y.inverse_transform(y_train.reshape(-1, 1)).clip(0, 20)\n",
    "# y_valid = scaler_y.inverse_transform(y_valid.reshape(-1, 1)).clip(0, 20)\n",
    "\n",
    "# train_pred = scaler_y.inverse_transform(train_pred.reshape(-1, 1)).clip(0, 20)\n",
    "# valid_pred = scaler_y.inverse_transform(valid_pred.reshape(-1, 1)).clip(0, 20)\n",
    "\n",
    "mean_cv_scores = [\n",
    "    rmse(y_train, train_pred),\n",
    "    rmse(y_valid, valid_pred),\n",
    "#   rmse(test_y, lstm.model.predict(test_X)),\n",
    "]\n",
    "mean_cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = pd.read_pickle('D:/ml_data/data_lstm_X_scaled.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_y_test(data, width=7):\n",
    "#     data.fillna(0, inplace=True)\n",
    "    \n",
    "#     test_data = data[(data.date_block_num >= (34 - width)) & (data.date_block_num < 35)]\n",
    "#     X = np.array(list(test_data.groupby(['shop_id', 'item_id']).apply(pd.DataFrame.as_matrix)))\n",
    "#     del test_data\n",
    "    \n",
    "#     X, y = window_stack(X, width=width)\n",
    "#     test_X = X[:, :, 3:]\n",
    "#     test_y = y[:, 1:4]\n",
    "#     del X\n",
    "#     del y\n",
    "    X = data[(data.date_block_num > 0) & (data.date_block_num < 34)]\n",
    "    X = np.array(list(X.groupby(['shop_id', 'item_id']).apply(pd.DataFrame.as_matrix)))\n",
    "    X = X[:, :, 3:]\n",
    "    y = data[data.date_block_num == 34][['shop_id', 'item_id', 'item_cnt_month']]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "ts = time.time()\n",
    "test_X, test_y = get_X_y_test(matrix, width)\n",
    "\n",
    "# train_data_file = 'train_data_Xscaled_{}.npz'.format(width)\n",
    "# test_data_file = 'test_data_Xscaled_{}.npz'.format(width)\n",
    "\n",
    "# np.savez_compressed(train_data_file, X=train_X, y=train_y)\n",
    "# np.savez_compressed(test_data_file, X=test_X, y=test_y)\n",
    "\n",
    "# del test_X\n",
    "# del test_y\n",
    "\n",
    "print(time.time() - ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_data_Xscaled_28.npz'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.load(test_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_data['X']\n",
    "test_y = test_data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_df = pd.DataFrame(data=test_y, columns=['shop_id', 'item_id', 'item_cnt_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from module.data.read_data import *\n",
    "\n",
    "test = test_file_processing().set_index('ID')\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[['shop_id', 'item_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pred = model.predict(test_X)\n",
    "test_y['item_cnt_month'] = _pred.clip(0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = pd.merge(test, test_y,  how='left', on=['shop_id', 'item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"ID\": answer.index, \n",
    "    \"item_cnt_month\": answer['item_cnt_month']\n",
    "})\n",
    "\n",
    "submission.fillna(0, inplace=True)\n",
    "# submission['ID'] = submission['ID'].astype(np.int32)\n",
    "submission['item_cnt_month'] = submission['item_cnt_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(_pred, open('lstm_test.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('lstm_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
